
<!DOCTYPE html>


<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>polylearn.factorization_machine &mdash; polylearn 0.1.0 documentation</title>
    
    <link rel="stylesheet" href="../../_static/basic.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/lightning.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/bootstrap.min.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/bootstrap-3.3.6/css/bootstrap.min.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/bootstrap-3.3.6/css/bootstrap-theme.min.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/bootstrap-sphinx.css" type="text/css" />
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '../../',
        VERSION:     '0.1.0',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="../../_static/jquery.js"></script>
    <script type="text/javascript" src="../../_static/underscore.js"></script>
    <script type="text/javascript" src="../../_static/doctools.js"></script>
    <script type="text/javascript" src="../../_static/js/jquery-1.11.0.min.js"></script>
    <script type="text/javascript" src="../../_static/js/jquery-fix.js"></script>
    <script type="text/javascript" src="../../_static/bootstrap-3.3.6/js/bootstrap.min.js"></script>
    <script type="text/javascript" src="../../_static/bootstrap-sphinx.js"></script>
    <link rel="top" title="polylearn 0.1.0 documentation" href="../../index.html" />
    <link rel="up" title="Module code" href="../index.html" />
<meta charset='utf-8'>
<meta http-equiv='X-UA-Compatible' content='IE=edge,chrome=1'>
<meta name='viewport' content='width=device-width, initial-scale=1.0, maximum-scale=1'>
<meta name="apple-mobile-web-app-capable" content="yes">

  </head>
  <body role="document">

  <div id="navbar" class="navbar navbar-default navbar-fixed-top">
    <div class="container">
      <div class="navbar-header">
        <!-- .btn-navbar is used as the toggle for collapsed navbar content -->
        <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".nav-collapse">
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand" href="../../index.html">
          polylearn</a>
        <span class="navbar-text navbar-version pull-left"><b>0.1</b></span>
      </div>

        <div class="collapse navbar-collapse nav-collapse">
          <ul class="nav navbar-nav">
            
                <li><a href="../../references.html">References</a></li>
                <li><a href="../../auto_examples/index.html">Examples</a></li>
            
            
              <li class="dropdown globaltoc-container">
  <a role="button"
     id="dLabelGlobalToc"
     data-toggle="dropdown"
     data-target="#"
     href="../../index.html">Site <b class="caret"></b></a>
  <ul class="dropdown-menu globaltoc"
      role="menu"
      aria-labelledby="dLabelGlobalToc"><ul>
<li class="toctree-l1"><a class="reference internal" href="../../auto_examples/index.html">Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../references.html">polylearn reference</a></li>
</ul>
</ul>
</li>
              
            
            
            
            
            
              <li class="hidden-sm"></li>
            
          </ul>

          
            
<form class="navbar-form navbar-right" action="../../search.html" method="get">
 <div class="form-group">
  <input type="text" name="q" class="form-control" placeholder="Search" />
 </div>
  <input type="hidden" name="check_keywords" value="yes" />
  <input type="hidden" name="area" value="default" />
</form>
          
        </div>
    </div>
  </div>

<div class="container content-container">
  
  <h1>Source code for polylearn.factorization_machine</h1><div class="highlight"><pre>
<span></span><span class="c1"># encoding: utf-8</span>

<span class="c1"># Author: Vlad Niculae &lt;vlad@vene.ro&gt;</span>
<span class="c1"># License: Simplified BSD</span>

<span class="kn">import</span> <span class="nn">warnings</span>
<span class="kn">from</span> <span class="nn">abc</span> <span class="k">import</span> <span class="n">ABCMeta</span><span class="p">,</span> <span class="n">abstractmethod</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="k">import</span> <span class="n">add_dummy_feature</span>
<span class="kn">from</span> <span class="nn">sklearn.utils</span> <span class="k">import</span> <span class="n">check_random_state</span>
<span class="kn">from</span> <span class="nn">sklearn.utils.validation</span> <span class="k">import</span> <span class="n">check_array</span>
<span class="kn">from</span> <span class="nn">sklearn.utils.extmath</span> <span class="k">import</span> <span class="n">safe_sparse_dot</span><span class="p">,</span> <span class="n">row_norms</span>
<span class="kn">from</span> <span class="nn">sklearn.externals</span> <span class="k">import</span> <span class="n">six</span>

<span class="k">try</span><span class="p">:</span>
    <span class="kn">from</span> <span class="nn">sklearn.exceptions</span> <span class="k">import</span> <span class="n">NotFittedError</span>
<span class="k">except</span> <span class="ne">ImportError</span><span class="p">:</span>
    <span class="k">class</span> <span class="nc">NotFittedError</span><span class="p">(</span><span class="ne">ValueError</span><span class="p">,</span> <span class="ne">AttributeError</span><span class="p">):</span>
        <span class="k">pass</span>

<span class="kn">from</span> <span class="nn">lightning.impl.dataset_fast</span> <span class="k">import</span> <span class="n">get_dataset</span>

<span class="kn">from</span> <span class="nn">.base</span> <span class="k">import</span> <span class="n">_BasePoly</span><span class="p">,</span> <span class="n">_PolyClassifierMixin</span><span class="p">,</span> <span class="n">_PolyRegressorMixin</span>
<span class="kn">from</span> <span class="nn">.kernels</span> <span class="k">import</span> <span class="n">_poly_predict</span>
<span class="kn">from</span> <span class="nn">.cd_direct_fast</span> <span class="k">import</span> <span class="n">_cd_direct_ho</span>


<span class="k">class</span> <span class="nc">_BaseFactorizationMachine</span><span class="p">(</span><span class="n">six</span><span class="o">.</span><span class="n">with_metaclass</span><span class="p">(</span><span class="n">ABCMeta</span><span class="p">,</span> <span class="n">_BasePoly</span><span class="p">)):</span>

    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">degree</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="s1">&#39;squared&#39;</span><span class="p">,</span> <span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">beta</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="mi">1</span><span class="n">e</span><span class="o">-</span><span class="mi">6</span><span class="p">,</span> <span class="n">fit_lower</span><span class="o">=</span><span class="s1">&#39;explicit&#39;</span><span class="p">,</span> <span class="n">fit_linear</span><span class="o">=</span><span class="s1">&#39;explicit&#39;</span><span class="p">,</span>
                 <span class="n">warm_start</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">init_lambdas</span><span class="o">=</span><span class="s1">&#39;random_signs&#39;</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span>
                 <span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">compute_loss</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">degree</span> <span class="o">=</span> <span class="n">degree</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">loss</span> <span class="o">=</span> <span class="n">loss</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_components</span> <span class="o">=</span> <span class="n">n_components</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">=</span> <span class="n">alpha</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">beta</span> <span class="o">=</span> <span class="n">beta</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tol</span> <span class="o">=</span> <span class="n">tol</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fit_lower</span> <span class="o">=</span> <span class="n">fit_lower</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fit_linear</span> <span class="o">=</span> <span class="n">fit_linear</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">warm_start</span> <span class="o">=</span> <span class="n">warm_start</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_lambdas</span> <span class="o">=</span> <span class="n">init_lambdas</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_iter</span> <span class="o">=</span> <span class="n">max_iter</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span> <span class="o">=</span> <span class="n">verbose</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">compute_loss</span> <span class="o">=</span> <span class="n">compute_loss</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">random_state</span> <span class="o">=</span> <span class="n">random_state</span>

    <span class="k">def</span> <span class="nf">_augment</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="c1"># for factorization machines, we add a dummy column for each order</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">fit_lower</span> <span class="o">==</span> <span class="s1">&#39;augment&#39;</span><span class="p">:</span>
            <span class="n">k</span> <span class="o">=</span> <span class="mi">1</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">fit_linear</span> <span class="o">==</span> <span class="s1">&#39;augment&#39;</span> <span class="k">else</span> <span class="mi">2</span>
            <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">degree</span> <span class="o">-</span> <span class="n">k</span><span class="p">):</span>
                <span class="n">X</span> <span class="o">=</span> <span class="n">add_dummy_feature</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">X</span>

    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Fit factorization machine to training data.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : array-like or sparse, shape = [n_samples, n_features]</span>
<span class="sd">            Training vectors, where n_samples is the number of samples</span>
<span class="sd">            and n_features is the number of features.</span>

<span class="sd">        y : array-like, shape = [n_samples]</span>
<span class="sd">            Target values.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        self : Estimator</span>
<span class="sd">            Returns self.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">degree</span> <span class="o">&gt;</span> <span class="mi">3</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;FMs with degree &gt;3 not yet supported.&quot;</span><span class="p">)</span>

        <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_check_X_y</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
        <span class="n">X</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_augment</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="n">n_features</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>  <span class="c1"># augmented</span>
        <span class="n">X_col_norms</span> <span class="o">=</span> <span class="n">row_norms</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">squared</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">dataset</span> <span class="o">=</span> <span class="n">get_dataset</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">order</span><span class="o">=</span><span class="s2">&quot;fortran&quot;</span><span class="p">)</span>
        <span class="n">rng</span> <span class="o">=</span> <span class="n">check_random_state</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">random_state</span><span class="p">)</span>
        <span class="n">loss_obj</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_loss</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">warm_start</span> <span class="ow">and</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;w_&#39;</span><span class="p">)):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">w_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_features</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">double</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">fit_lower</span> <span class="o">==</span> <span class="s1">&#39;explicit&#39;</span><span class="p">:</span>
            <span class="n">n_orders</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">degree</span> <span class="o">-</span> <span class="mi">1</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">n_orders</span> <span class="o">=</span> <span class="mi">1</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">warm_start</span> <span class="ow">and</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;P_&#39;</span><span class="p">)):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">P_</span> <span class="o">=</span> <span class="mf">0.01</span> <span class="o">*</span> <span class="n">rng</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n_orders</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_components</span><span class="p">,</span> <span class="n">n_features</span><span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">warm_start</span> <span class="ow">and</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;lams_&#39;</span><span class="p">)):</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">init_lambdas</span> <span class="o">==</span> <span class="s1">&#39;ones&#39;</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">lams_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_components</span><span class="p">)</span>
            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">init_lambdas</span> <span class="o">==</span> <span class="s1">&#39;random_signs&#39;</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">lams_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sign</span><span class="p">(</span><span class="n">rng</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_components</span><span class="p">))</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Lambdas must be initialized as ones &quot;</span>
                                 <span class="s2">&quot;(init_lambdas=&#39;ones&#39;) or as random &quot;</span>
                                 <span class="s2">&quot;+/- 1 (init_lambdas=&#39;random_signs&#39;).&quot;</span><span class="p">)</span>

        <span class="n">y_pred</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_output</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

        <span class="n">converged</span> <span class="o">=</span> <span class="n">_cd_direct_ho</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">P_</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">w_</span><span class="p">,</span> <span class="n">dataset</span><span class="p">,</span> <span class="n">X_col_norms</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span>
                                  <span class="n">y_pred</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">lams_</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">degree</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span><span class="p">,</span>
                                  <span class="bp">self</span><span class="o">.</span><span class="n">beta</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">fit_linear</span> <span class="o">==</span> <span class="s1">&#39;explicit&#39;</span><span class="p">,</span>
                                  <span class="bp">self</span><span class="o">.</span><span class="n">fit_lower</span> <span class="o">==</span> <span class="s1">&#39;explicit&#39;</span><span class="p">,</span> <span class="n">loss_obj</span><span class="p">,</span>
                                  <span class="bp">self</span><span class="o">.</span><span class="n">max_iter</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">tol</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">,</span>
                                  <span class="bp">self</span><span class="o">.</span><span class="n">compute_loss</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">converged</span><span class="p">:</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;Objective did not converge. Increase max_iter.&quot;</span><span class="p">)</span>

        <span class="k">return</span> <span class="bp">self</span>

    <span class="k">def</span> <span class="nf">_get_output</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="n">y_pred</span> <span class="o">=</span> <span class="n">_poly_predict</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">P_</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:],</span> <span class="bp">self</span><span class="o">.</span><span class="n">lams_</span><span class="p">,</span> <span class="n">kernel</span><span class="o">=</span><span class="s1">&#39;anova&#39;</span><span class="p">,</span>
                               <span class="n">degree</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">degree</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">fit_linear</span><span class="p">:</span>
            <span class="n">y_pred</span> <span class="o">+=</span> <span class="n">safe_sparse_dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">w_</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">fit_lower</span> <span class="o">==</span> <span class="s1">&#39;explicit&#39;</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">degree</span> <span class="o">==</span> <span class="mi">3</span><span class="p">:</span>
            <span class="c1"># degree cannot currently be &gt; 3</span>
            <span class="n">y_pred</span> <span class="o">+=</span> <span class="n">_poly_predict</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">P_</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:],</span> <span class="bp">self</span><span class="o">.</span><span class="n">lams_</span><span class="p">,</span>
                                    <span class="n">kernel</span><span class="o">=</span><span class="s1">&#39;anova&#39;</span><span class="p">,</span> <span class="n">degree</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">y_pred</span>

    <span class="k">def</span> <span class="nf">_predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;P_&quot;</span><span class="p">):</span>
            <span class="k">raise</span> <span class="n">NotFittedError</span><span class="p">(</span><span class="s2">&quot;Estimator not fitted.&quot;</span><span class="p">)</span>
        <span class="n">X</span> <span class="o">=</span> <span class="n">check_array</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">accept_sparse</span><span class="o">=</span><span class="s1">&#39;csc&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">double</span><span class="p">)</span>
        <span class="n">X</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_augment</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_output</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>


<div class="viewcode-block" id="FactorizationMachineRegressor"><a class="viewcode-back" href="../../generated/polylearn.FactorizationMachineRegressor.html#polylearn.FactorizationMachineRegressor">[docs]</a><span class="k">class</span> <span class="nc">FactorizationMachineRegressor</span><span class="p">(</span><span class="n">_BaseFactorizationMachine</span><span class="p">,</span>
                                    <span class="n">_PolyRegressorMixin</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Higher order factorization machine for regression (with squared loss).</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>

<span class="sd">    degree : int &gt;= 2, default: 2</span>
<span class="sd">        Degree of the polynomial. Corresponds to the order of feature</span>
<span class="sd">        interactions captured by the model. Currently only supports</span>
<span class="sd">        degrees up to 3.</span>

<span class="sd">    n_components : int, default: 2</span>
<span class="sd">        Number of basis vectors to learn, a.k.a. the dimension of the</span>
<span class="sd">        low-rank parametrization.</span>

<span class="sd">    alpha : float, default: 1</span>
<span class="sd">        Regularization amount for linear term (if ``fit_linear=True``).</span>

<span class="sd">    beta : float, default: 1</span>
<span class="sd">        Regularization amount for higher-order weights.</span>

<span class="sd">    tol : float, default: 1e-6</span>
<span class="sd">        Tolerance for the stopping condition.</span>

<span class="sd">    fit_lower : {&#39;explicit&#39;|&#39;augment&#39;|None}, default: &#39;explicit&#39;</span>
<span class="sd">        Whether and how to fit lower-order, non-homogeneous terms.</span>

<span class="sd">        - &#39;explicit&#39;: fits a separate P directly for each lower order.</span>

<span class="sd">        - &#39;augment&#39;: adds the required number of dummy columns (columns</span>
<span class="sd">           that are 1 everywhere) in order to capture lower-order terms.</span>

<span class="sd">        - None: only learns weights for the degree given.</span>

<span class="sd">    fit_linear : {&#39;explicit&#39;|&#39;augment&#39;|None}, default: &#39;explicit&#39;</span>
<span class="sd">        Whether and how to fit a linear term to the model.</span>

<span class="sd">        - &#39;explicit&#39;: fits w in &lt;X, w&gt; directly using coordinate descent.</span>
<span class="sd">          This works even if ``fit_lower is None``.</span>

<span class="sd">        - &#39;augment&#39;: adds an extra dummy column to account for the linear</span>
<span class="sd">          term. This only works if ``fit_lower=&#39;augment&#39;``.</span>

<span class="sd">        - None: does not learn a linear term at all.</span>

<span class="sd">    warm_start : boolean, optional, default: False</span>
<span class="sd">        Whether to use the existing solution, if available. Useful for</span>
<span class="sd">        computing regularization paths or pre-initializing the model.</span>

<span class="sd">    init_lambdas : {&#39;ones&#39;|&#39;random_signs&#39;}, default: &#39;random_signs&#39;</span>
<span class="sd">        How to initialize the predictive weights of each learned basis. The</span>
<span class="sd">        lambdas are not trained; using alternate signs can theoretically</span>
<span class="sd">        improve performance if the kernel degree is even.</span>

<span class="sd">    max_iter : int, optional, default: 10000</span>
<span class="sd">        Maximum number of passes over the dataset to perform.</span>

<span class="sd">    verbose : boolean, optional, default: False</span>
<span class="sd">        Whether to print debugging information.</span>

<span class="sd">    compute_loss : boolean, optional, default: False</span>
<span class="sd">        Whether to compute the training loss at every iteration. Slows down</span>
<span class="sd">        runtime but may be useful for debugging convergence issues.</span>

<span class="sd">    random_state : int seed, RandomState instance, or None (default)</span>
<span class="sd">        The seed of the pseudo random number generator to use for</span>
<span class="sd">        initializing the parameters.</span>

<span class="sd">    Attributes</span>
<span class="sd">    ----------</span>

<span class="sd">    self.P_ : array, shape [n_orders, n_components, n_features]</span>
<span class="sd">        The learned basis functions.</span>

<span class="sd">        self.P_[0, :, :] is always available, and corresponds to</span>
<span class="sd">        interactions of order ``self.degree``.</span>

<span class="sd">        self.P_[i, :, :] for i &gt; 0 corresponds to interactions of order</span>
<span class="sd">        ``self.degree - i``, available only if ``self.fit_lower=&#39;explicit&#39;``.</span>

<span class="sd">    self.w_ : array, shape [n_features]</span>
<span class="sd">        The learned linear model, completing the FM.</span>

<span class="sd">        Only present if ``self.fit_linear=&#39;explicit&#39;``</span>

<span class="sd">    self.lams_ : array, shape [n_components]</span>
<span class="sd">        The predictive weights.</span>

<span class="sd">    References</span>
<span class="sd">    ----------</span>
<span class="sd">    Polynomial Networks and Factorization Machines:</span>
<span class="sd">    New Insights and Efficient Training Algorithms.</span>
<span class="sd">    Mathieu Blondel, Masakazu Ishihata, Akinori Fujino, Naonori Ueda.</span>
<span class="sd">    In: Proceedings of ICML 2016.</span>
<span class="sd">    http://mblondel.org/publications/mblondel-icml2016.pdf</span>

<span class="sd">    Factorization machines.</span>
<span class="sd">    Steffen Rendle</span>
<span class="sd">    In: Proceedings of IEEE 2010.</span>
<span class="sd">    &quot;&quot;&quot;</span>
<div class="viewcode-block" id="FactorizationMachineRegressor.__init__"><a class="viewcode-back" href="../../generated/polylearn.FactorizationMachineRegressor.html#polylearn.FactorizationMachineRegressor.__init__">[docs]</a>    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">degree</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="mi">1</span><span class="n">e</span><span class="o">-</span><span class="mi">6</span><span class="p">,</span>
                 <span class="n">fit_lower</span><span class="o">=</span><span class="s1">&#39;explicit&#39;</span><span class="p">,</span> <span class="n">fit_linear</span><span class="o">=</span><span class="s1">&#39;explicit&#39;</span><span class="p">,</span> <span class="n">warm_start</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">init_lambdas</span><span class="o">=</span><span class="s1">&#39;random_signs&#39;</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">compute_loss</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>

        <span class="nb">super</span><span class="p">(</span><span class="n">FactorizationMachineRegressor</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">__init__</span><span class="p">(</span>
            <span class="n">degree</span><span class="p">,</span> <span class="s1">&#39;squared&#39;</span><span class="p">,</span> <span class="n">n_components</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="n">tol</span><span class="p">,</span> <span class="n">fit_lower</span><span class="p">,</span>
            <span class="n">fit_linear</span><span class="p">,</span> <span class="n">warm_start</span><span class="p">,</span> <span class="n">init_lambdas</span><span class="p">,</span> <span class="n">max_iter</span><span class="p">,</span> <span class="n">verbose</span><span class="p">,</span>
            <span class="n">compute_loss</span><span class="p">,</span> <span class="n">random_state</span><span class="p">)</span></div></div>


<div class="viewcode-block" id="FactorizationMachineClassifier"><a class="viewcode-back" href="../../generated/polylearn.FactorizationMachineClassifier.html#polylearn.FactorizationMachineClassifier">[docs]</a><span class="k">class</span> <span class="nc">FactorizationMachineClassifier</span><span class="p">(</span><span class="n">_BaseFactorizationMachine</span><span class="p">,</span>
                                     <span class="n">_PolyClassifierMixin</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Higher order factorization machine for classification.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>

<span class="sd">    degree : int &gt;= 2, default: 2</span>
<span class="sd">        Degree of the polynomial. Corresponds to the order of feature</span>
<span class="sd">        interactions captured by the model. Currently only supports</span>
<span class="sd">        degrees up to 3.</span>

<span class="sd">    loss : {&#39;logistic&#39;|&#39;squared_hinge&#39;|&#39;squared&#39;}, default: &#39;squared_hinge&#39;</span>
<span class="sd">        Which loss function to use.</span>

<span class="sd">        - logistic: L(y, p) = log(1 + exp(-yp))</span>

<span class="sd">        - squared hinge: L(y, p) = max(1 - yp, 0)²</span>

<span class="sd">        - squared: L(y, p) = 0.5 * (y - p)²</span>

<span class="sd">    n_components : int, default: 2</span>
<span class="sd">        Number of basis vectors to learn, a.k.a. the dimension of the</span>
<span class="sd">        low-rank parametrization.</span>

<span class="sd">    alpha : float, default: 1</span>
<span class="sd">        Regularization amount for linear term (if ``fit_linear=True``).</span>

<span class="sd">    beta : float, default: 1</span>
<span class="sd">        Regularization amount for higher-order weights.</span>

<span class="sd">    tol : float, default: 1e-6</span>
<span class="sd">        Tolerance for the stopping condition.</span>

<span class="sd">    fit_lower : {&#39;explicit&#39;|&#39;augment&#39;|None}, default: &#39;explicit&#39;</span>
<span class="sd">        Whether and how to fit lower-order, non-homogeneous terms.</span>

<span class="sd">        - &#39;explicit&#39;: fits a separate P directly for each lower order.</span>

<span class="sd">        - &#39;augment&#39;: adds the required number of dummy columns (columns</span>
<span class="sd">           that are 1 everywhere) in order to capture lower-order terms.</span>

<span class="sd">        - None: only learns weights for the degree given.</span>

<span class="sd">    fit_linear : {&#39;explicit&#39;|&#39;augment&#39;|None}, default: &#39;explicit&#39;</span>
<span class="sd">        Whether and how to fit a linear term to the model.</span>

<span class="sd">        - &#39;explicit&#39;: fits w in &lt;X, w&gt; directly using coordinate descent.</span>
<span class="sd">          This works even if fit_lower is None.</span>

<span class="sd">        - &#39;augment&#39;: adds an extra dummy column to account for the linear</span>
<span class="sd">          term. This only works if ``fit_lower=&#39;augment&#39;``.</span>

<span class="sd">        - None: does not learn a linear term at all.</span>

<span class="sd">    warm_start : boolean, optional, default: False</span>
<span class="sd">        Whether to use the existing solution, if available. Useful for</span>
<span class="sd">        computing regularization paths or pre-initializing the model.</span>

<span class="sd">    init_lambdas : {&#39;ones&#39;|&#39;random_signs&#39;}, default: &#39;random_signs&#39;</span>
<span class="sd">        How to initialize the predictive weights of each learned basis. The</span>
<span class="sd">        lambdas are not trained; using alternate signs can theoretically</span>
<span class="sd">        improve performance if the kernel degree is even.</span>

<span class="sd">    max_iter : int, optional, default: 10000</span>
<span class="sd">        Maximum number of passes over the dataset to perform.</span>

<span class="sd">    verbose : boolean, optional, default: False</span>
<span class="sd">        Whether to print debugging information.</span>

<span class="sd">    compute_loss : boolean, optional, default: False</span>
<span class="sd">        Whether to compute the training loss at every iteration. Slows down</span>
<span class="sd">        runtime but may be useful for debugging convergence issues.</span>

<span class="sd">    random_state : int seed, RandomState instance, or None (default)</span>
<span class="sd">        The seed of the pseudo random number generator to use for</span>
<span class="sd">        initializing the parameters.</span>

<span class="sd">    Attributes</span>
<span class="sd">    ----------</span>

<span class="sd">    self.P_ : array, shape [n_orders, n_components, n_features]</span>
<span class="sd">        The learned basis functions.</span>

<span class="sd">        self.P_[0, :, :] is always available, and corresponds to</span>
<span class="sd">        interactions of order ``self.degree``.</span>

<span class="sd">        self.P_[i, :, :] for i &gt; 0 corresponds to interactions of order</span>
<span class="sd">        ``self.degree - i``, available only if ``self.fit_lower=&#39;explicit&#39;``.</span>

<span class="sd">    self.w_ : array, shape [n_features]</span>
<span class="sd">        The learned linear model, completing the FM.</span>

<span class="sd">        Only present if ``self.fit_linear=&#39;explicit&#39;``</span>

<span class="sd">    self.lams_ : array, shape [n_components]</span>
<span class="sd">        The predictive weights.</span>

<span class="sd">    References</span>
<span class="sd">    ----------</span>
<span class="sd">    Polynomial Networks and Factorization Machines:</span>
<span class="sd">    New Insights and Efficient Training Algorithms.</span>
<span class="sd">    Mathieu Blondel, Masakazu Ishihata, Akinori Fujino, Naonori Ueda.</span>
<span class="sd">    In: Proceedings of ICML 2016.</span>
<span class="sd">    http://mblondel.org/publications/mblondel-icml2016.pdf</span>

<span class="sd">    Factorization machines.</span>
<span class="sd">    Steffen Rendle</span>
<span class="sd">    In: Proceedings of IEEE 2010.</span>
<span class="sd">    &quot;&quot;&quot;</span>

<div class="viewcode-block" id="FactorizationMachineClassifier.__init__"><a class="viewcode-back" href="../../generated/polylearn.FactorizationMachineClassifier.html#polylearn.FactorizationMachineClassifier.__init__">[docs]</a>    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">degree</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="s1">&#39;squared_hinge&#39;</span><span class="p">,</span> <span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">beta</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="mi">1</span><span class="n">e</span><span class="o">-</span><span class="mi">6</span><span class="p">,</span> <span class="n">fit_lower</span><span class="o">=</span><span class="s1">&#39;explicit&#39;</span><span class="p">,</span> <span class="n">fit_linear</span><span class="o">=</span><span class="s1">&#39;explicit&#39;</span><span class="p">,</span>
                 <span class="n">warm_start</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">init_lambdas</span><span class="o">=</span><span class="s1">&#39;random_signs&#39;</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span>
                 <span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">compute_loss</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>

        <span class="nb">super</span><span class="p">(</span><span class="n">FactorizationMachineClassifier</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">__init__</span><span class="p">(</span>
            <span class="n">degree</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span> <span class="n">n_components</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="n">tol</span><span class="p">,</span> <span class="n">fit_lower</span><span class="p">,</span>
            <span class="n">fit_linear</span><span class="p">,</span> <span class="n">warm_start</span><span class="p">,</span> <span class="n">init_lambdas</span><span class="p">,</span> <span class="n">max_iter</span><span class="p">,</span> <span class="n">verbose</span><span class="p">,</span>
            <span class="n">compute_loss</span><span class="p">,</span> <span class="n">random_state</span><span class="p">)</span></div></div>
</pre></div>

</div>
<footer class="footer">
  <div class="container">
    <p class="pull-right">
      <a href="#">Back to top</a>
      
    </p>
    <p>
        &copy; Copyright 2016, Vlad Niculae.<br/>
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 1.4.5.<br/>
    </p>
  </div>
</footer>
  </body>
</html>